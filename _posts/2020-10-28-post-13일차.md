---
title: "인공지능_자연어처리_2_13일차"
categories:
  - AI
tags:
  - AI
  - NLP
---

## 인공지능 자연어처리 2 (13일차)

- Relu
    - 0 이하의 값은 다 0으로 된다.
    - 미분을 하면, 0 보다 큰 구간에서는 뒤에서 날라온 값은 그대로 사용하고
      0보다 작은 구간에서는 0으로 만든다.
    - 모든 음수는 0으로 해준다. backward 시에도 음수에 대한 미분 값은 0으로 해준다.
```python
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x<=0)
        print(self.mask)
        out = x.copy()
        out[self.mask] = 0
        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout;
        return dx
```

- Sigmoid 계층
    - ** Sigmoid 는 미분하기 위해 들어온 X, W, B 를 다 기억해야 한다.
    - .dot 의 미분은 해당 입력 값으로 편미분하고 남은 것을 전치 하고 이전
      미분값을 해당 자리에 두고 곱하면 된다.
    - B 의 미분을 하기 위해서는 forward 일때 (3,) 을 repeat 해서 더했기 때문에
      backward 시 미분은 세로의 합이기 때문에 세로로 더해서 구한다.
      (repeat 의 미분은 sum 이다!!)

```python
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = 1/(1+np.exp(-x))
        self.out = out
        return out

    def backward(self, dout):
        dx = dout*self.out*(1.0-self.out);
        return dx
```




python 의 boolean index ??
Affine : 행렬의 곱 ??
