---
title: "인공지능_자연어처리_2_14일차"
categories:
  - AI
tags:
  - AI
  - NLP
---

## 인공지능 자연어처리 2 (14일차)

- +, X, .dot 의 역전파 시의 방법과 기억하는 것들을 잘 알아야 한다.
    - + : repeat
    - X : 번갈아서 곱해준다.
    - .dot : 곱해준다.

- 멀티플 : x, y를 기억한다.

- + : 는 기억할것이 없다.

- ReLU : 음수는 0으로 만든다. 자기의 객체에 구현된 값이 음수일때를 기억한다.


- Sigmoid : 그래프 이론으로 만들면 이해하기 쉽다. Sigmod 의 결과 값을 기억한다.
    - 분수의 미분을 "/" (인버스) 이다.
    - forward : sigmoid 값.
    - backward : sigmoid 의 미분값.


- Affine : 행렬의 곱을 알아야 한다. B를 기억한다.
    - B : 세로로 합친 값을 미분한다.(repeat 의 미분값은 합이)


- Sofmax with Loss
  cross_entropy_error 와 softmax 를 같이 하면,
  예측값-실제값이 미분이다.
  예측값과 실제값을 기억한다.


----

- SGD 문제점  : x, y 의 기울기가 비슷하면 문제가 없지만, 어느 한쪽의 경사가 급하면
              문제점이 생긴다. 그래서 v(속도)를 w에 더하는 Momentum을 사용한다.


- AdaGrad : h를 곱해서 학습률을 낮춘다.
  RMSprop : AdaGrad 와 비슷하다. decay_rate가 추가 된거다.


- Adam : 가장 최신이다.

모든 optimizer는 update()를 구현해야 한다.

- 초기화의 중요성.
Sigmod 시 기울기의 소식을 막기 위해 0.01 을 random()에 곱하면,  
다양하게 분포가 되지 않으면 학습이 잘 안된다.  
그래서 이것을 최대한 넓게 펴주는게 좋다.  
입력층의 node_num가 중요하다. 해당 노드수에 루트를 적용한 후 나누어 주면 분포가 다양하게 펼쳐진다.  
(사비에르 글로로트가 주장함.)  
tanh 도 사비에르 글로로트 주장을 적용해도 좋아 진다.  
단! ReLU 에는 적용하면 한쪽으로 몰리기 때문에 안 좋아 진다.  
ReLU 에서는 분자에 2.0 곱하기를 하여 음수 영역을 더 넓게 만들어 분포를 고르게 한다.  
(카이밍 히 가 주장함.)  

---
배치 정규화

"Batch Norm" 기법을 하면 초기화 최적화를 하지 않아도 된다.

평균,
분산,
표준편차(분산에 루트를 적용하면 표준편차 이다.)

중간중간 데이터를 흩뜨려주는 동작을 해서 효과가 좋다.!!

단, 역전파 할때는 노드를 추가 했기 때문에 어렵다.

orderedict ? 순서대로 dictionary 를 만든다.
sqrt  는 루트.
