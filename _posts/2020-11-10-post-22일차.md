---
title: "인공지능_자연어처리_3_22일차"
categories:
  - AI
tags:
  - AI
  - NLP
---

## 인공지능 자연어처리 3 (22일차)

### RNN
시간의 순번을 옆으로 구현...
출력층으로 가는 값을 다음으로 넘겨준다.

ht = tanh(ht-1Wh + xtWx + b)

페176
입력값 x 는 각 단어,
위쪽에는 소프트맥스가 있다.
백워드 일때는 위에서 온값과 뒤에서 온값을 더해서 구한다.

페177
너무 많이 가면 오버헤드가 있으니.. Truncated BPTT 를 하자
(뒤에서 오는 미분을 전달하는 것을 짜른다.)

페187
hnext 위에서 오는 값과 뒤에서 오는 값은 나갈때는 tanh 값으로 같아서 repeat 하고
미분할때는 전달해 오는 값은 다른데.. 그값은 repeat의 미분인 합을 하면 된다.

페188
단일 RNN 안에 미분을 구현해 두고..

### Time RNN
입력값을 바로 RNN으로 넣지 않고, 밀집백터(wordvecsize)로 구현한 값을 입력한다.
입력값의 숫자와 밀집백터의 행수는 값아야 하고,

x 는 밀집백터(분산표현)를 넣는 것이다.

time_layers.py 에 Embedding 소스가 있다.

페190
xs (샘플수N, 타임수T, 밀집백터수D)
hs (샘플수N, 타임수T, 셀의수H)  


### 미니배치 수의 상관 관계
- 학습 시 사용하는 변수
max_intes 부터 상관이 있다.
max_intes = data_size // batch_size * time_size
1 = 7 // 2 * 3

- 모델 생성
미니배치 수와는 상관 없다.

- 미니배치의 각 샘플의 읽기 시작 위치를 계산
미니배치가 2일때 jump 가 3이 된다.
offsets 은 [0, 3] 이 된다.

- 미니배치 취득
batch_x : (2, 3) 빈 행렬을 만든다.
batch_t :

- timeembedding
Embedding 에서 하나가 뽑혀 나오는게 아니다.

- forward
(2,4)(4,3)+(2,3)(3,3) = (2,3)
       셀수

2차원으로 변경한다.
xs = np.array(24).reshape(2,3,4)
x = xs[:, 0, :]


페230
MatMul 은 형상도 바뀌지만, 그 속에 있는 값이 계속 곱해지기 때문에
기울기의 폭박이 발생할 수 있다.!!!

### LSTM
각 W 값이 중요하다.
W 를 압축해서 사용하는 형식으로 한다.

LSTM 에서 사용 되는 W 는 하나더라..


tanh 미분은 1-tanh**2 이고 이것의 인자는 포워드시 입력한 값이 인자값이다.

i * (1 - i) : 시그모이드 미분.

W를 모아서 forward 시에는 slice 로 f,g,i,o 로 나눠 사용을 한다.
backward 시에는 f,g,i,o 를 합해서 slice 를 만든다.

c 는 위 아래로 전파가 안되기 때문에 입력할때랑 뒤로 보낼때만 신경 써주면 된다.

LSTM 을 진짜로 구동 시키는 것은 Time LSTM 이다

소스 6-3
h와 c 는 초기에는 0으로 시작 한다.

backward 시에는 dc 는 0으로 시작하고, dh는 오른쪽에서 들어오는건 0이지만,
위에층에서 들어오는게 있기때문에 dh 는 0으로 시작하지 않는다.

밑으로 보낼때도 모아서 dx를 모아 dxs를 보낸다.
